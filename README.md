# U-Net Implementation from Scratch

A comprehensive implementation of U-Net and hybrid ResNet+U-Net models for semantic segmentation, built from scratch using PyTorch.

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
  - [U-Net](#u-net)
  - [ResNet+U-Net Hybrid](#resnetunet-hybrid)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Model Architecture Details](#model-architecture-details)
- [Training](#training)
- [Results](#results)
- [File Structure](#file-structure)
- [Requirements](#requirements)
- [Contributing](#contributing)
- [License](#license)

## Overview

This repository contains a from-scratch implementation of U-Net, a fully convolutional network designed for semantic segmentation tasks. Additionally, we provide a hybrid architecture that combines ResNet backbone with U-Net decoder for improved feature extraction and segmentation performance.

U-Net was introduced in the paper ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/abs/1505.04597) by Ronneberger et al. (2015), and has become a standard architecture for image segmentation in medical imaging and other domains.

## Architecture

### U-Net

The classical U-Net architecture consists of:

- **Encoder (Contracting Path)**: A series of convolutional blocks that progressively downsample the input image while increasing the number of feature channels. Each block typically contains:
  - Two 3Ã—3 convolutional layers
  - ReLU activation functions
  - Max pooling (2Ã—2) for downsampling

- **Bottleneck**: The deepest layer where the spatial dimensions are smallest but feature richness is highest

- **Decoder (Expanding Path)**: A series of upsampling blocks that progressively upsample the feature maps while decreasing the number of channels. Each block contains:
  - Upsampling (transposed convolution or interpolation)
  - Concatenation with corresponding encoder feature maps (skip connections)
  - Two 3Ã—3 convolutional layers
  - ReLU activation functions

- **Final Output Layer**: A 1Ã—1 convolution that maps feature maps to the desired number of classes

#### Key Characteristics:
- **Skip Connections**: Direct connections between encoder and decoder at each level preserve spatial information and aid gradient flow
- **Symmetric Architecture**: The decoder mirrors the encoder structure
- **End-to-End Training**: Fully convolutional approach allowing variable input sizes (with padding considerations)

### ResNet+U-Net Hybrid

The hybrid model combines the strengths of both architectures:

- **Encoder**: ResNet backbone (ResNet18, ResNet34, ResNet50, etc.) for powerful feature extraction
  - Pre-trained weights can be optionally loaded for transfer learning
  - Extracts multi-scale features at different depth levels

- **Decoder**: U-Net-style decoder with skip connections
  - Receives feature maps from multiple ResNet stages
  - Progressive upsampling with feature concatenation
  - Reduces channels progressively toward output

#### Advantages:
- Better feature representation through ResNet's residual connections
- Leverages pre-trained ImageNet weights for improved performance
- Maintains the benefit of U-Net's skip connections for detailed segmentation
- More efficient training on limited datasets

## Features

âœ¨ **Key Features:**

- ğŸ“ **Pure PyTorch Implementation**: No reliance on external segmentation libraries
- ğŸ—ï¸ **Multiple Architecture Options**: Classic U-Net and ResNet+U-Net hybrid
- ğŸ”„ **Skip Connections**: Efficient gradient flow and spatial information preservation
- ğŸ“Š **Flexible Input Sizes**: Handles various image dimensions
- ğŸ¯ **Multi-class Segmentation**: Support for binary and multi-class segmentation tasks
- ğŸš€ **Transfer Learning Ready**: Optional pre-trained weights for ResNet backbone
- ğŸ“ˆ **Training Scripts**: Complete training pipeline with validation
- ğŸ’¾ **Model Checkpointing**: Save and load trained models
- ğŸ“‰ **Comprehensive Logging**: Track training metrics and visualizations

## Installation

```bash
# Clone the repository
git clone https://github.com/ComputerFish/U-Net-from-scratch.git
cd U-Net-from-scratch

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## Usage

### Basic U-Net Model

```python
import torch
from models.unet import UNet

# Create a U-Net model
# Parameters: in_channels (input channels), out_channels (output classes), features (base feature count)
model = UNet(in_channels=3, out_channels=1, features=64)

# Forward pass
x = torch.randn(1, 3, 572, 572)  # Batch, Channels, Height, Width
output = model(x)
print(output.shape)  # torch.Size([1, 1, 388, 388])
```

### ResNet+U-Net Hybrid Model

```python
import torch
from models.resnet_unet import ResNetUNet

# Create ResNet+U-Net hybrid model
# Parameters: num_classes, pretrained, depth (18, 34, 50, etc.)
model = ResNetUNet(num_classes=1, pretrained=True, depth=50)

# Forward pass
x = torch.randn(1, 3, 512, 512)
output = model(x)
print(output.shape)  # torch.Size([1, 1, 512, 512])
```

### Training

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from models.unet import UNet

# Initialize model
model = UNet(in_channels=3, out_channels=1, features=64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Loss and optimizer
criterion = nn.BCEWithLogitsLoss()  # For binary segmentation
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop
epochs = 50
for epoch in range(epochs):
    model.train()
    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)
        
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, masks)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

# Save model
torch.save(model.state_dict(), "unet_model.pth")
```

## Model Architecture Details

### U-Net Architecture

```
Input (572x572x3)
    â†“
[Conv 3x3 + ReLU] Ã— 2 â†’ (570x570x64)
    â†“ MaxPool 2x2
[Conv 3x3 + ReLU] Ã— 2 â†’ (284x284x128)
    â†“ MaxPool 2x2
[Conv 3x3 + ReLU] Ã— 2 â†’ (140x140x256)
    â†“ MaxPool 2x2
[Conv 3x3 + ReLU] Ã— 2 â†’ (68x68x512)
    â†“ MaxPool 2x2
[Conv 3x3 + ReLU] Ã— 2 â†’ (32x32x1024)  [Bottleneck]
    â†‘
[UpConv] â†’ concat with encoder feature map
[Conv 3x3 + ReLU] Ã— 2 â†’ (68x68x512)
    â†‘
[UpConv] â†’ concat with encoder feature map
[Conv 3x3 + ReLU] Ã— 2 â†’ (140x140x256)
    â†‘
[UpConv] â†’ concat with encoder feature map
[Conv 3x3 + ReLU] Ã— 2 â†’ (284x284x128)
    â†‘
[UpConv] â†’ concat with encoder feature map
[Conv 3x3 + ReLU] Ã— 2 â†’ (572x572x64)
    â†“
Conv 1x1 â†’ (572x572x1)  [Output]
```

### ResNet+U-Net Hybrid Architecture

```
Input (512x512x3)
    â†“
ResNet Encoder (Pre-trained backbone)
â”œâ”€ Layer 1: stride-1 â†’ features[64]
â”œâ”€ Layer 2: stride-2 â†’ features[128]
â”œâ”€ Layer 3: stride-4 â†’ features[256]
â””â”€ Layer 4: stride-8 â†’ features[512]
    â†“
U-Net Decoder with Skip Connections
â”œâ”€ Upsample + concat [512 features]
â”œâ”€ [Conv blocks + ReLU]
â”œâ”€ Upsample + concat [256 features]
â”œâ”€ [Conv blocks + ReLU]
â”œâ”€ Upsample + concat [128 features]
â”œâ”€ [Conv blocks + ReLU]
â”œâ”€ Upsample + concat [64 features]
â”œâ”€ [Conv blocks + ReLU]
â””â”€ Final Conv 1x1
    â†“
Output (512x512xnum_classes)
```

## Training

Refer to the `train.py` script for a complete training pipeline including:
- Data loading and preprocessing
- Model initialization
- Loss function selection
- Optimization strategies
- Validation and metric calculation
- Checkpoint saving

### Key Training Considerations:

1. **Loss Functions**:
   - Binary Cross-Entropy with Logits (BCEWithLogitsLoss) for binary segmentation
   - Cross-Entropy Loss (CrossEntropyLoss) for multi-class segmentation
   - Dice Loss for imbalanced datasets

2. **Optimization**:
   - Adam optimizer recommended for most tasks
   - Learning rate: typically 1e-4 to 1e-3
   - Learning rate scheduling for improved convergence

3. **Data Augmentation**:
   - Random rotations
   - Elastic deformations
   - Brightness/contrast adjustments
   - Horizontal/vertical flips

4. **Batch Size**: 2-16 depending on GPU memory and image size

## Results

Results will depend on your specific dataset and training configuration. Typical performance metrics include:
- **Dice Coefficient**: Measures overlap between predicted and ground truth segmentations
- **IoU (Intersection over Union)**: Computes the ratio of true positives
- **Pixel Accuracy**: Percentage of correctly classified pixels

## File Structure

```
U-Net-from-scratch/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unet.py              # Classic U-Net implementation
â”‚   â””â”€â”€ resnet_unet.py       # ResNet+U-Net hybrid model
â”œâ”€â”€ train.py                 # Training script
â”œâ”€â”€ evaluate.py              # Evaluation script
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py       # Data loading utilities
â”‚   â””â”€â”€ metrics.py           # Evaluation metrics
â””â”€â”€ examples/
    â””â”€â”€ example_usage.py     # Usage examples
```

## Requirements

- Python 3.7+
- PyTorch 1.9+
- torchvision
- NumPy
- Pillow
- tqdm (for progress bars)

See `requirements.txt` for specific versions.

## Contributing

Contributions are welcome! Please feel free to:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## References

- **Original U-Net Paper**: Ronneberger, O., Fischer, P., & Brox, T. (2015). "U-Net: Convolutional Networks for Biomedical Image Segmentation." MICCAI 2015.
- **ResNet Paper**: He, K., Zhang, X., Ren, S., & Sun, J. (2015). "Deep Residual Learning for Image Recognition." CVPR 2015.

---

**Created by**: ComputerFish  
**Last Updated**: January 2026

For questions or issues, please open an GitHub issue or contact the repository maintainer.
